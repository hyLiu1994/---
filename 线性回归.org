* 线性回归
** 问题定义
数据的表示形式如下:
\begin{equation}
\label{eq:1}
\mathbf{X} = [\mathbf{x_1}, \mathbf{x_2}, \cdots, \mathbf{x_N}]^T
=\left[\begin{array}{c}
\mathbf{x_1^T} \\
\mathbf{x_2^T} \\
\vdots\\
\mathbf{x_N^T} 
\end{array}\right]
\end{equation}
\begin{equation}
\label{eq:2}
$$\mathbf{Y} = \left[\begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{array}\right]$$
\end{equation}
** 最小二乘法
*** 最小二乘法推导
最小二乘法的损失函数如下所示:
\begin{equation}
\label{eq:3}
\mathcal{L}(\mathbf{W}) = \sum_{i=1}^N||\mathbf{W}^T\mathbf{x}_i - y_i||
\end{equation}
也即是求
$$W_{LSE} = \argmin_{\mathbf{W}}\sum_{i=1}^N||\mathbf{W}^T\mathbf{x}_i - y_i||$$

矩阵表达如下
\begin{equation}
\label{eq:8}
\mathcal{L}(\mathbf{w}) = \left[\mathbf{XW} - \mathbf{Y}\right]^T\left[\mathbf{XW} - \mathbf{Y}\right]
\end{equation}
令 $\mathbf{Z} = \mathbf{XW} - \mathbf{Y}$, $L(\mathbf{w}) = \mathbf{Z}^T \mathbf{Z}$ , $\frac{d L(\mathbf{W})}{d \mathbf{W}} = \frac{d L(\mathbf{W})}{d \mathbf{Z}} \frac{d \mathbf{Z}}{d \mathbf{W}}$ 。

由矩阵求导法则
\begin{equation}
\label{eq:4}
\begin{align}
$$\frac{d L(\mathbf{W})}{d \mathbf{Z}} &= 2 \mathbf{Z}^T\\
$$\frac{d \mathbf{Z}}{d \mathbf{W}} &= \mathbf{X}\\
\end{align}
\end{equation}

所以
\begin{equation}
\label{eq:6}
\begin{align}
\label{eq:7}
\frac{d L(\mathbf{W})}{d \mathbf{W}} &= \frac{d L(\mathbf{W})}{d \mathbf{Z}} \frac{d \mathbf{Z}}{d \mathbf{W}} = 2 \mathbf{Z}^T \mathbf{X} = 2[\mathbf{XW} - \mathbf{Y}]^T\mathbf{X} = 0\\
&[\mathbf{W}^T \mathbf{X}^T - \mathbf{Y}^T] \mathbf{X} = 0 \\
&\mathbf{W}^T \mathbf{X}^T \mathbf{X} - \mathbf{Y}^T \mathbf{X} = 0 \\ 
&\mathbf{W}^T \mathbf{X}^T \mathbf{X} = \mathbf{Y}^T \mathbf{X} \\
&\mathbf{W}^T  = \mathbf{Y}^T \mathbf{X} \left(\mathbf{X}^T \mathbf{X}\right)^{-1}\\
&\mathbf{W}  = \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T \mathbf{Y}
\end{align}
\end{equation}

*** 最小二乘法几何解释 
\begin{equation}
\label{eq:5}
\mathcal{L}(\mathbf{w}) = \left[\mathbf{XW} - \mathbf{Y}\right]^T \left[\mathbf{XW} - \mathbf{Y}\right]
\end{equation}
**** 数据的表示形式 
$$
\mathbf{X} = [\mathbf{X_1}, \mathbf{X_2}, \cdots, \mathbf{X_N}]^T
=\left[\begin{array}{c}
\mathbf{X_1^T} \\
\mathbf{X_2^T} \\
\vdots\\
\mathbf{X_N^T} 
\end{array}\right] = \left[\begin{array}{cccc}
x_{11}, & x_{12}, & \cdots, & x_{1p}\\
x_{21}, & x_{22}, & \cdots, & x_{2p}\\ 
\vdots  &         & \ddots \\
x_{N1}, & x_{N2}, & \cdots, & x_{NP}
\end{array}\right]
$$

**** 列向量空间
首先，引入求解线性方程组中的列空间的概念。
$$\mathbf{Ax = b}, \mathbf{A} \in \mathbb{R}_{m \times n}, \mathbf{x} \in \mathbb{R}_{n \times 1}, \mathbf{b} \in \mathbb{R}_{m \times 1}$$
矩阵$\mathbf{A}$ 可以分解为列向量的组合 
$$\mathbf{A} = [\mathbf{c}_1, \mathbf{c}_2, \cdots, \mathbf{c}_n]$$
$\mathbf{x}$ 为列向量, $$\mathbf{x} = [x_1, x_2, \cdots, x_n]^T = \left[\begin{array}{c}x_1 \\ x_2 \\ \vdots \\ x_n \end{array}\right]$$ 
也即是
$$\mathbf{Ax} = [\mathbf{c}_1, \mathbf{c}_2, \cdots, \mathbf{c}_n] \left[\begin{array}{c}x_1 \\ x_2 \\ \vdots \\ x_n \end{array}\right] = x_1 \mathbf{c}_1 + x_2 \mathbf{c}_2 + \cdots + x_n \mathbf{c}_n$$

$\mathbf{Ax}$ 的会对 $\mathbf{A}$ 所有列进行线性组合，若 $\mathbf{A}$ 中的列向量线性无关的数量为 $r$, $r \leq \min(m, n)$, 为方便讨论，这里假设$r=n$. $\mathbf{Ax}$ 对应的所有的向量，也即是，以 $r$ 个线性无关向量所有线性组合对应的向量空间,这里记为 $V$ 。

易知，若向量 $\mathbf{b}$ 位于线性空间 $V$ 中，那么存在线性组合
$$\mathbf{x}^{'} = \left[\begin{array}{c}x^{'}_1\\ x^{'}_2\\ \vdots \\ x^{'}_n \end{array}\right]$$
使得 $$x^{'}_1 \mathbf{c}_1 + x^{'}_2 \mathbf{c}_2 + \cdots + x^{'}_n \mathbf{c}_n = \mathbf{b}$$, $\mathbf{x^{'}}$ 即为方程组的解。
可通过如下例子，加深对列空间的理解
假设
$$
\mathbf{A} = \left[\begin{array}{c}
1 & 2 \\
2 & 3 \\
0 & 0 \\ 
\end{array}\right] 
$$
$\mathbf{A}$ 可由如下列向量组成 $\mathbf{v}_1 = \left[\begin{array}{c} 1 \\ 2 \\ 0 \end{array}\right]$, $\mathbf{v}_2 = \left[\begin{array}{c} 2 \\ 3 \\ 0 \end{array}\right]$, 若将向量的三个维度分别对应 $x, y, z$ 轴，则 $v_1, v_2$ 两个不共线向量，表示了 $xy$ 平面，在该平面任取一个向量作 $\mathbf{b}$ ，例如 $\mathbf{b}_{in} = [3, 5, 0]^T$ 都可以得到对应的解，但是若对第三维度添加噪音$-1$ ，获得新的$\mathbf{b}_{not} = [3, 5, -1]$,该向量不再 $xy$ 平面，自然不存在相应的解。但是对于 $\mathbf{b}_{not}$ 最优估计应是 $\mathbf{b}_{not}$ 在 $xy$ 平面的投影 $\mathbf{b}_{in}$. 更高维度可视作超平面作类似推导。

**** 从列向量空间角度解释最小二乘法
借鉴列向量空间中 $\mathbf{AX=b}$ 相关推导，可对当前问题作如下推导，当前问题中 $\mathbf{X}, \mathbf{W}, \mathbf{Y}$ 分别与 $\mathbf{AX=b}$ 问题中的 $\mathbf{A}, \mathbf{x}, \mathbf{b}$ 对应
所以
$$\mathbf{XW} =
w_1 \left[\begin{array}{c} 
x_{11}\\ 
x_{21}\\
\vdots \\
x_{N1}
\end{array}\right]
+w_2 \left[\begin{array}{c} 
x_{12}\\ 
x_{22}\\
\vdots \\
x_{N2}
\end{array}
\right]
+\cdots
+w_p \left[\begin{array}{c} 
x_{12}\\ 
x_{22}\\
\vdots \\
x_{Np}
\end{array}
\right]
= w_1 \mathbf{v_1} + w_2 \mathbf{v_2} + \cdots + w_p \mathbf{v_p}
$$
求解 $Y$ 找出向量 $\mathbf{v_1}, \mathbf{v_2},\cdots, \mathbf{v_p}$ 线性组合中最接近 $Y$ 的向量。也即是 $Y$ 在 $\mathbf{v_1}, \mathbf{v_2},\cdots, \mathbf{v_p}$ 向量空间中的投影：
$$\mathbf{X}^T(\mathbf{Y} - \mathbf{XW}) = \mathbf{0}$$
这里作如下几何意义的解释：$w_1 \mathbf{v_1} + w_2 \mathbf{v_2} + \cdots + w_p \mathbf{v_p}$

$\mathbf{W}$ 对 $\mathbf{X}$ 进行线性组合得到对 $\mathbf{Y}$ 的预测，我们期望对 $Y$ 的拟合，偏差尽可能的小，也即是
$$\mathbf{X}^T\mathbf{Y} - \mathbf{X}^T\mathbf{XW} = \mathbf{0}$$
$$\mathbf{X}^T\mathbf{XW} = \mathbf{X}^T\mathbf{Y}$$
$$\mathbf{W} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$$
*从列向量空间角度讲,最小二乘法实际上将误差均匀的平分在每一个特征上.*
**** 总结
*从最直观的角度讲，最小二乘法实际上将误差均匀的平分在每一个样本点上.*
*从列向量空间角度讲,最小二乘法实际上将误差均匀的平分在每一个特征上.*
** 概率视角的最小二乘法---带有高斯噪声的最大似然估计
*** 最小二乘法求解 
由前节推导可知，问题描述
损失函数方程如下
$$L(\mathbf{W}) = \sum_{i=1}^N||\mathbf{W}^T\mathbf{x}_i - y_i||$$
也即是求
$$W_{LSE} = \argmin_{\mathbf{W}}\sum_{i=1}^N||\mathbf{W}^T\mathbf{x}_i - y_i||$$
$$
\mathbf{X} = [\mathbf{x_1}, \mathbf{x_2}, \cdots, \mathbf{x_N}]^T
=\left[\begin{array}{c}
\mathbf{x_1^T} \\
\mathbf{x_2^T} \\
\vdots \\
\mathbf{x_N^T} 
\end{array}\right] = \left[\begin{array}{cccc}
x_{11}, & x_{12}, & \cdots, & x_{1p}\\
x_{21}, & x_{22}, & \cdots, & x_{2p}\\ 
\vdots  &         & \ddots \\
x_{N1}, & x_{N2}, & \cdots, & x_{NP}
\end{array}\right]
$$
*** 最大似然估计求解 
假设 $\varepsilon \sim N(0, \sigma^2)$ 为随机噪声，$Y_i = \mathbf{W}^T\mathbf{x}_i + \varepsilon$
所以 $Y_i|\mathbf{x}_i, \mathbf{W} \sim N(\mathbf{W}^T\mathbf{x}_i, \sigma^2)$
即 $$p(y_i|\mathbf{x}_i, \mathbf{W}) = \frac{1}{\sqrt{2\pi \sigma}} \exp\{- \frac{(y_i - \mathbf{W}^T\mathbf{x}_i)^2}{2 \sigma ^ 2}\}$$
似然函数如下
$$\begin{aligned}
\mathcal{L}(\mathbf{W}) &= \log \mathbf{P}(\mathbf{Y}|\mathbf{X},\mathbf{W}) \\
&=\log \prod_{i=1}^N p(y_i|\mathbf{x}_i, \mathbf{W})\\
&=\sum_{i=1}^N\left[\log\frac{1}{\sqrt{2\pi \sigma}} - \frac{(y_i - \mathbf{W}^T \mathbf{x}_i)^2}{2 \sigma^2}\right]
\end{aligned}$$

$$\begin{aligned}
\mathbf{W}_{MLE} &= \arg\max_{\mathbf{W}} \mathcal{L}(\mathbf{W})\\
&=\arg\max_{\mathbf{W}} \sum_{i=1}^N\left[\log\frac{1}{\sqrt{2\pi \sigma}} - \frac{(y_i - \mathbf{W}^T \mathbf{x}_i)^2}{2 \sigma^2}\right] \\
&=\arg\max_{\mathbf{W}} \sum_{i=1}^N - \frac{(y_i - \mathbf{W}^T \mathbf{x}_i)^2}{2 \sigma^2} \\
&=\arg\max_{\mathbf{W}} \sum_{i=1}^N - (y_i - \mathbf{W}^T \mathbf{x}_i)^2\\
&=\arg\min_{\mathbf{W}} \sum_{i=1}^N (y_i - \mathbf{W}^T \mathbf{x}_i)^2\\
\end{aligned}$$

由此可知，若假设噪声为 $\varepsilon$ 服从正态分布，则最小二乘法和最大似然估计求解效果一致，即:
若 $Y = \mathbf{W}^T\mathbf{X} + \varepsilon$, 其中 $\varepsilon \sim N(0, \sigma)$ ,则$\mathbf{W}_{LSE} = \mathbf{W}_{MLE}$

** 正则化-岭回归
对于最小二乘法
\begin{equation}
\label{eq:9}
L(\mathbf{W}) = \sum_{i=1}^N||\mathbf{W}^T\mathbf{x}_i - y_i||
\end{equation}
\begin{equation}
\label{eq:11}
\mathbf{W}_{LSE} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
\end{equation}
其中 $\mathbf{X}_{N \times P}$ , 样本数为$N$, 特征数量为$P$, 一般$P \ll N$ . 
- 若 $N < p$ ，则 $\mathbf{X}^T\mathbf{X}$ 存在不可逆的情况
- 若 $N < p$ ，会发生过拟合  

过拟合一般解决办法如下
- 增加数据
- 降维(特征选择/特征提取([[file:%E9%99%8D%E7%BB%B4.org::*%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%20(PCA)][主成分分析 (PCA)]]))
- 正则化(参数空间的约束)

对于线性回归，正则化框架如下

\begin{equation}
\label{eq:10}
\mathbf{W}_{Ridge Regression} = \argmin_{\mathbf{W}} \sum_{i=1}^N \left[(y_i - \mathbf{W}^T \mathbf{x}_i)^2 + \lambda \mathbf{W}^T \mathbf{W}\right]
\end{equation}
矩阵表达如下
\begin{equation}
\label{eq:12}
L(\mathbf{W}) = [\mathbf{XW} - \mathbf{Y}]^T[\mathbf{XW} - \mathbf{Y}] + \lambda \mathbf{W}^T\mathbf{W}
\end{equation}
\begin{equation}
\label{eq:13}
\begin{aligned}\frac{d L(\mathbf{W})}{d \mathbf{W}} &= 2(\mathbf{XW} - \mathbf{Y})^T \mathbf{X} + 2\lambda \mathbf{W}^T = 0 &\\
&\Rightarrow (\mathbf{W}^T\mathbf{X}^T - \mathbf{Y}^T)\mathbf{X} + \lambda \mathbf{W}^T = 0 &\\
&\Rightarrow \mathbf{W}^T\mathbf{X}^T \mathbf{X} - \mathbf{Y}^T\mathbf{X} + \lambda \mathbf{W}^T = 0 &\\
&\Rightarrow \mathbf{W}^T(\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})  =\mathbf{Y}^T\mathbf{X} &\\
&\Rightarrow \mathbf{W}^T =\mathbf{Y}^T\mathbf{X} (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1}&\\
&\Rightarrow \mathbf{W} =(\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T\mathbf{Y}&\\
\end{aligned}
\end{equation}
** 概率角度下的正则化
*这里的 $\mathbf{W}, \mathbf{x}_i$ 看作一维向量*
*** 以贝叶斯角度转化问题
假设 $\mathbf{W}$ 的先验分布： $$\mathbf{W} \sim N(\mathbf{0}, \sigma_w^2)$$
$$\mathbf{Y} = \mathbf{W}^T \mathbf{X} + \varepsilon$$

$$Y_i|\mathbf{x}_i,\mathbf{W} \sim N(\mathbf{W}^T\mathbf{x}_i, \sigma^2)$$

由此可得
$$p(y_i|\mathbf{x}_i, \mathbf{W}) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\{- \frac{(y_i - \mathbf{W}^T \mathbf{x}_i)^2}{2 \sigma^2}\}$$

依据贝叶斯定理
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
*** 似然函数推导
因 $\mathbf{x}_i$ 为常量(观测量)，所以
$$ P(Y_i|\mathbf{W}) = \sum_{\mathbf{x}}P(Y_i|\mathbf{x}_i,\mathbf{W}) = P(Y_i|\mathbf{x}_i, \mathbf{W})$$
所以
$$p(y_i|\mathbf{W}) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\{- \frac{(y_i - \mathbf{W}^T \mathbf{x}_i)^2}{2 \sigma^2}\}$$

因为样本之间独立同分布，所以
$$
P(\mathbf{Y}|\mathbf{W}) = \prod_{i=1}^N P(Y_i|\mathbf{W})
$$

所以
$$\begin{aligned}
P(\mathbf{Y}|\mathbf{W}) &= \prod_{i=1}^N p(y_i|\mathbf{W}) \\
&= \prod_{i=1}^N p(y_i|\mathbf{x}_i, \mathbf{W})
\end{aligned}$$

由前面假设可知
$$p(y_i|\mathbf{W}) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\{- \frac{(y_i - \mathbf{W}^T \mathbf{x}_i)^2}{2 \sigma^2}\}$$
$$p(\mathbf{w}) = \frac{1}{\sqrt{2\pi \sigma_w^2}} \exp\{- \frac{||\mathbf{w}||^2}{2 \sigma_w^2}\}$$
*** 最大后验概率(MAP)
$$\begin{aligned}
\mathbf{w}_{map} 
&= \arg \max_{\mathbf{w}} \prod_{i = 1}^n p(\mathbf{w}|y_i) \\
&\propto \arg \max_{\mathbf{w}} \prod_{i = 1}^n p(y_i|\mathbf{w})p(\mathbf{w}) \\
&\propto \arg \max_{\mathbf{w}} \sum_{i = 1}^n \log \left[ p(y_i|\mathbf{w})p(\mathbf{w})\right] \\
&= \arg \arg \max_{\mathbf{W}} \sum_{i=1}^N \log \left[\frac{1}{\sqrt{2\pi \sigma}} \frac{1}{\sqrt{2\pi \sigma_w}} \exp \{- \frac{(y_i - \mathbf{W}^T \mathbf{x}_i)^2 }{2 \sigma^2} - \frac{||\mathbf{w}||^2}{2 \sigma_w^2}\}\right] \\
&= \arg \max_{\mathbf{W}} \sum_{i=1}^N \left[\log \frac{1}{\sqrt{2\pi \sigma}} + \log \frac{1}{\sqrt{2\pi \sigma_w}} - \frac{(y_i - \mathbf{W}^T \mathbf{x}_i)^2 }{2 \sigma^2} - \frac{||\mathbf{w}||^2}{2 \sigma_w^2}\right]\\
&= \argmax_{\mathbf{W}} \sum_{i=1}^N \left[ - \frac{(y_i - \mathbf{W}^T \mathbf{x}_i)^2 }{2 \sigma^2} - \frac{||\mathbf{w}||^2}{2 \sigma_w^2}\right]\\
&= \argmin_{\mathbf{W}} \sum_{i=1}^N \left[\frac{(y_i - \mathbf{W}^T \mathbf{x}_i)^2 }{2 \sigma^2} + \frac{||\mathbf{w}||^2}{2 \sigma_w^2}\right]\\
&= \argmin_{\mathbf{W}} \sum_{i=1}^N \left[(y_i - \mathbf{W}^T \mathbf{x}_i)^2 + \frac{2 \sigma^2}{2 \sigma_w^2} ||\mathbf{w}||^2 \right]\\
\end{aligned}
$$

*** 总结
$$\mathbf{W}_{MAP} = \argmin_{\mathbf{W}} \sum_{i=1}^N \left[(y_i - \mathbf{W}^T \mathbf{x}_i)^2 + \frac{2 \sigma^2}{2 \sigma_w^2} ||\mathbf{w}||^2 \right]$$
$$\mathbf{W}_{Ridge Regression} = \argmin_{\mathbf{W}} \sum_{i=1}^N \left[(y_i - \mathbf{W}^T \mathbf{x}_i)^2 + \lambda \mathbf{W}^T \mathbf{W}\right]$$
可得出如下结论:
*正则化的LSE $\Leftrightarrow$  MAP（ $\mathbf{W}$ 先验分布为高斯分布，噪声为高斯分布)*


