[toc]
# 背景介绍
首先是对于过拟合问题, 解决方案如下:
- 增加数据
- 正则化
- 降维
  - 直接降维：特征选择(例如正则化中的lasso, 部分特征系数为0)
  - 线性降维：PCA, MDS(Multidimensional Scaling)
  - 非线性降维
    - 流形
      - ISOMAP
      - LLE
维度过高造成的问题： 数据稀疏性
**解释**
# 样本方差, 样本均值 矩阵表示
$$
\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_N]^T
=\left[\begin{array}{c}
\mathbf{x}_1^T \\
\mathbf{x}_2^T \\
\vdots\\
\mathbf{x}_N^T
\end{array}\right] = \left[\begin{array}{cccc}
x_{11}, & x_{12}, & \cdots, & x_{1p}\\
x_{21}, & x_{22}, & \cdots, & x_{2p}\\ 
\vdots  &         & \ddots \\
x_{N1}, & x_{N2}, & \cdots, & x_{NP}
\end{array}\right]
$$ 
## 样本均值
$$
\begin{aligned}
\overline{\mathbf{x}} &= \frac{1}{N} \sum_{i=1}^N \mathbf{x}_i \\
&= \frac{1}{N}[\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_N]\left[\begin{array}{c} \mathbf{1} \\\mathbf{1} \\ \vdots \\ \mathbf{1}\end{array}\right] \\
&= \frac{1}{N} \mathbf{X}^T \large{\mathbf{1}}
\end{aligned}
$$
$$
\begin{aligned}
\mathbf{S} &= \frac{1}{N} \sum_{i=1}^N(\mathbf{x}_i - \overline{\mathbf{x}})(\mathbf{x}_i - \overline{\mathbf{x}})^T \\
&= \frac{1}{N} [\mathbf{x}_1 - \overline{\mathbf{x}}, \quad \mathbf{x}_2 - \overline{\mathbf{x}}, \quad \cdots, \quad \mathbf{x}_N - \overline{\mathbf{x}}]
\left[\begin{array}{c}
(\mathbf{x}_1 - \overline{\mathbf{x}})^T\\ 
(\mathbf{x}_2 - \overline{\mathbf{x}})^T\\ 
\vdots \\ 
(\mathbf{x}_N - \overline{\mathbf{x}})^T
\end{array} \right]\\
&=\frac{1}{N} \mathbf{X}_{centralized} \mathbf{X}_{centralized}^T
\end{aligned}
$$
对于
$$
\mathbf{X}_{centralized} = [\mathbf{x}_1 - \overline{\mathbf{x}}, \quad \mathbf{x}_2 - \overline{\mathbf{x}}, \quad \cdots, \quad \mathbf{x}_N - \overline{\mathbf{x}}]
$$
作如下变换
$$
\begin{aligned}
\mathbf{X}_{centralized}
&=[\mathbf{x}_1 - \overline{\mathbf{x}}, \quad \mathbf{x}_2 - \overline{\mathbf{x}}, \quad \cdots, \quad \mathbf{x}_N - \overline{\mathbf{x}}] \\
&= [\mathbf{x}_1 - \overline{\mathbf{x}}, \quad \mathbf{x}_2 - \overline{\mathbf{x}}, \quad \cdots, \quad \mathbf{x}_N - \overline{\mathbf{x}}] \\
&= [\mathbf{x}_1, \quad \mathbf{x}_2, \quad \cdots, \quad \mathbf{x}_N] - [\overline{\mathbf{x}}, \quad \overline{\mathbf{x}}, \quad \cdots, \quad \overline{\mathbf{x}}] \\
&= [\mathbf{x}_1, \quad \mathbf{x}_2, \quad \cdots, \quad \mathbf{x}_N] - \overline{\mathbf{x}}[1, \quad 1, \quad \cdots, \quad 1] \\
&= [\mathbf{x}_1, \quad \mathbf{x}_2, \quad \cdots, \quad \mathbf{x}_N] - \overline{\mathbf{x}} \large{\mathbf{1}}^T\\
&= \mathbf{X}^T - \overline{\mathbf{x}} \large{\mathbf{1}}^T \\
&= \mathbf{X}^T - \frac{1}{N} \mathbf{X}^T \large{\mathbf{1}}\large{\mathbf{1}}^T \\
&= \mathbf{X}^T(\mathbf{I} - \frac{1}{N} \large{\mathbf{1}}\large{\mathbf{1}}^T)
\end{aligned} 
$$
>记
$$\mathbf{H} = \mathbf{I} - \frac{1}{N} \large{\mathbf{1}}\large{\mathbf{1}}^T$$
为 中心化矩阵
易知 
$\mathbf{H}$ 为对称矩阵，即 $\mathbf{H}^T = \mathbf{H}$
且满足 $\mathbf{H} \mathbf{H} = \mathbf{H}$

所以
$$
\begin{aligned}
\mathbf{S} &= \frac{1}{N} \sum_{i=1}^N(\mathbf{x}_i - \overline{\mathbf{x}})(\mathbf{x}_i - \overline{\mathbf{x}})^T \\
&= \frac{1}{N} \mathbf{X}_{centralized} \mathbf{X}_{centralized}^T \\
&= \frac{1}{N} \mathbf{X}^T(\mathbf{I} - \frac{1}{N} \large{\mathbf{1}}\large{\mathbf{1}}^T) \\
&= \frac{1}{N} \mathbf{X}^T \mathbf{H} \mathbf{H}^T \mathbf{X}\\
&= \frac{1}{N} \mathbf{X}^T \mathbf{H} \mathbf{X}
\end{aligned}
$$
# PCA-最大投影方差角度
对于PCA,要点为"一个中心，两个基本点"：
- 一个中心：原始特征空间的重构
  - 特征: 相关性 $\rightarrow$ 无关性(例如 人的特征： 身高，体重 成正相关)
- 两个基本点：最大投影方差，最小重构距离(两者效果相同，只是一种方法的两个观察角度)
## 最大投影方差理解
最大投影方差，希望在一条直线上面的投影点，尽可能的分散，易于区分数据点。同时，从信息角度，方差越大，包含的信息越多。
## 最大投影方差步骤
### 数据中心化
$$\hat{\mathbf{x}}_i = \mathbf{x}_i - \overline{\mathbf{x}}$$
>如果$\mathbf{u}_i$为单位向量，则，向量$\mathbf{x}_i$在上面的投影为$\mathbf{x}_i \mathbf{u}_i$

$$\begin{aligned}
J
&=\frac{1}{N} \sum_{i=1}^{N} \left[(\mathbf{x}_i -\bar{\mathbf{x}})^{T} \mathbf{u}_1\right]^2, \quad \text { s.t. } \mathbf{u}_1^{T} \mathbf{u}_1 = 1\\
&=\frac{1}{N} \sum_{i=1}^{N}\left[\mathbf{u}_1^{T}\left(\mathbf{x}_{i}-\bar{\mathbf{x}}\right)\left(\mathbf{x}_{i}-\bar{\mathbf{x}}\right)^{T} \mathbf{u}_{1}\right]\\
&=\mathbf{u}_{1}^{T}\left[\frac{1}{N} \sum_{i=1}^{N}\left(\mathbf{x}_{i}-\bar{\mathbf{x}}\right)\left(\mathbf{x}_{i}-\bar{\mathbf{x}}\right)^{T}\right] \mathbf{u}_{1}\\
&=\mathbf{u}_{1}^{T} \mathbf{S} \mathbf{u}_{1}\\
\end{aligned}$$
$$
\left\{\begin{array}{l}
\hat{\mathbf{u}}_{1}=\operatorname{argmax} \mathbf{u}_{1}^{T} \Sigma \mathbf{u}_{1} \\
\text{s. t.} \quad \mathbf{u}_{1}^{T} \mathbf{u}_{1}=1
\end{array}\right.
$$
$$
L\left(\mathbf{u}_{1}, \lambda\right)=\mathbf{u}_{1}^{T} \mathbf{S} \mathbf{u}_{1}+\lambda\left(1-\mathbf{u}_{1}^{T} \mathbf{u}_{1}\right)
$$
$$
\begin{aligned} 
\frac{\partial L}{\partial \mathbf{u}_{1}}
&=2 \mathbf{S} \mathbf{u}_{1} - 2 \lambda \mathbf{u}_{1}=0 \\
& \Rightarrow \mathbf{S} \mathbf{u}_{1}= \lambda \mathbf{u}_{1}
\end{aligned}
$$
由此可知问题的求解转变为求解特征值，特征向量
# PCA-最小重构距离
首先，假设，数据空间可以基于如下向量$\mathbf{u}_1,\mathbf{u}_2,\cdots, \mathbf{u}_p$完整表示，我们选取$q$个特征作为有用的特征，同时要尽可能，保证，$q$个特征表示的数据与原有的数据表达相近。
$$
\begin{aligned}
\text{数据}\mathbf{x}_i\text{的完整表示:} &\mathbf{x}_i = \sum_{k=1}^p((\mathbf{x}_i - \overline{\mathbf{x}}_i) \mathbf{u}_k)\mathbf{u}_k \\
\text{选取}q\text{特征进行表示:} & \hat{\mathbf{x}}_i = \sum_{k=1}^q((\mathbf{x}_i - \overline{\mathbf{x}}_i)\mathbf{u}_k)\mathbf{u}_k
\end{aligned}
$$
## 最小重构距离推导
同最大投影方差一样进行一样的数据中心化，这里不再赘述。
$$\begin{aligned}
J &=\frac{1}{N} \sum_{i=1}^{N}\left\|\mathbf{x}_{i}-\hat{\mathbf{x}}_{i}\right\|^{2} \\
&=\frac{1}{N} \sum_{i=1}^{N}\left\|\sum_{k=q+1}^{p}\left((\mathbf{x}_{i}-\bar{\mathbf{x}})^T \mathbf{u}_{k}\right) \mathbf{u}_{k}\right\|^{2}\\
&=\frac{1}{N} \sum_{i=1}^{N} \sum_{k=q+1}^{p}\left[(\mathbf{x}_{i}-\bar{\mathbf{x}})^T \mathbf{u}_{k}\right]^{2} \\
&=\frac{1}{N} \sum_{i=1}^{N} \sum_{k=q+1}^{p} \left[(\mathbf{x}_{i}-\bar{\mathbf{x}})^T \mathbf{u}_{k}\right]^{2} \\
&=\sum_{k=q+1}^{p} \frac{1}{N} \sum_{i=1}^{N}\left[(\mathbf{x}_{i}-\bar{\mathbf{x}})^T \mathbf{u}_{k}\right]^{2} \\
&=\sum_{k=q+1}^{p} \mathbf{u}_{k}^T \mathbf{S} \mathbf{u}_{k}
\end{aligned}$$
可得如下表示
$$\left\{\begin{array}{l}
\mathbf{u}_{k}=\operatorname{argmin} \sum_{q+1}^{q} \mathbf{u}_{k}^T \mathbf{S} \mathbf{u}_{k} \\
\mathbf{u}_{k}^T \mathbf{u}_{k}=1
\end{array}\right.$$
这里可以看到，实际效果同最大投影方差效果一致，最大投影方差选取 $\operatorname{argmax} \mathbf{u}_{k}^T \mathbf{S} \mathbf{u}_{k}$ 作为投影向量; 最小重构距离删去 $\operatorname{argmin} \mathbf{u}_{k}^T \mathbf{S} \mathbf{u}_{k}$ 的向量。也即是，最大投影方差选取特征值最大的几个特征向量，最小重构距离删除特征值最小的几个向量。
>因为$\frac{\partial \mathbf{u}_{k}^T \mathbf{S} \mathbf{u}_{k}}{\partial \mathbf{u}_k} = 0 \Rightarrow \mathbf{S} \mathbf{u}_{k}= \lambda \mathbf{u}_{k} \Rightarrow \mathbf{u}_{k}^T \mathbf{S} \mathbf{u}_{k}= \mathbf{u}_{k}^T \lambda \mathbf{u}_{k} =  \lambda  \mathbf{u}_{k}^T \mathbf{u}_{k} =  \lambda, \text{因为}\mathbf{u}_{k}^T \mathbf{u}_{k} = 1$ 

# PCA-SVD角度
## $\mathbf{S} = \mathbf{GKG}$
$$\mathbf{S} = \mathbf{GKG}, \mathbf{G^TG = I}, \mathbf{k}=\left[\begin{array}{lll}
k_{1} & \\
& \ddots \\
& & k_{p}
\end{array}\right]$$
假设，我们已将 $k_1, k_2, \cdots, k_p$,从小到大排序，
则只需要选取特征值, $q$ 个最大特征值对应的特征向量用于数据的描述，即达到了从$p$到$q$的降维。
## $\mathbf{S} = \mathbf{X^THX}$
- 首先对中心化数据进行特征值分解
$$\mathbf{HX} = \mathbf{U \Sigma V^T}, \mathbf{U^TU=I}, \mathbf{V^TV=I}, \Sigma \text{对角阵}$$
- 对样本方差作如下推导
$$
\begin{aligned}
\mathbf{S}
&= \mathbf{X^THX} \\
&= \mathbf{X^TH^THX} \\
&= \mathbf{V \Sigma U U^T \Sigma V^T} \\
&= \mathbf{V \Sigma^2 V^T}
\end{aligned}
$$
也即是，若对中心化的数据进行特征值分解，$\mathbf{V}$ 即为样本方差 $\mathbf{S}$ 的特征值向量矩阵, $\mathbf{\Sigma}^2$ 即为特征值向量矩阵 $\mathbf{k}$ 
## $\mathbf{T} = \mathbf{HXX^TH}$
首先，对中心化数据作如下变换可得坐标矩阵
$$\mathbf{HXV = U \Sigma V^TV = U \Sigma}$$
$$\begin{aligned}
\mathbf{T}&=\mathbf{H X X^{T} H} \\
&=\mathbf{U \Sigma V^{T} V \Sigma U^{T}}\\
&=\mathbf{U \Sigma^{2} U^{T}}\\
&=\mathbf{U \Sigma (\Sigma U)^{T}}
\end{aligned}$$
由此可知对 $\mathbf{T}$ 分解得特征向量$\mathbf{U}$后，对$\mathbf{U}$ 做一定放缩($\mathbf{\Sigma}$)投影坐标矩阵.
# P-PCA
$$\begin{aligned}
&\mathbf{X} \in R^{p}, \mathbf{Z} \in R^{q}, q<p\\
&\left\{\begin{array}{l}
\mathbf{z} \sim N\left(\mathbf{0}_q, \mathbf{I}_{q}\right) \\
\mathbf{x}= \mathbf{w} \mathbf{z} + \mu + \varepsilon \\
\varepsilon \sim N\left(\mathbf{0}_p, \delta^{2} I_{p}\right)
\end{array}\right.\\
\end{aligned}$$
对于 $\mathbf{X|Z, X, Z|X}$分布的推导，参考高斯分布 