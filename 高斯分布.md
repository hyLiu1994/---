[toc]
##  高斯分布公式
1. 一维高斯分布公式
$$
   P(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp \left\{-\frac{(x-\mu)^2}{2\sigma^2} \right\}
$$
2. 多维高斯分布公式
$$
 P(x) = \frac{1}{(2\pi)^{\frac{1}{P}}\left| \Sigma \right|^{\frac{1}{2}}} \exp \left\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right\}
$$
$\Sigma$ 为协方差矩阵
##  问题定义
数据与参数定义为如下形式:
$$
Data : X = (x_1,x_2,...,x_N)^T = \left (\begin{array}{c}
x_{1}^T \\
x_2^T \\
... \\
x_N^T
\end{array}
\right )_{N \times P}
$$
\[
Parameter: \theta = (\mu,\Sigma)
\]

\[
x_i \in \mathbb{R}^{P}
\]
$$
\begin{aligned}
x_i \sim \mathcal{N}(\mu,\Sigma) = \frac{1}{\left( 2\pi \right)^{\frac{P}{2} \cdot \left| \Sigma \right|^{\frac{1}{2}}}} \exp \left( -\frac{1}{2} \left( x-\mu \right)^T \Sigma^{-1} \left( x - \mu \right) \right)
\end{aligned}
$$
$\left( x - \mu \right)^T \Sigma^{-1} \left( x - \mu \right)$ 为[二次型](./线性代数内容.org)。
$$
\begin{aligned}
x_i = \left ( 
\begin{array}{c}
x_{i,1} \\
x_{i,2} \\
... \\
x_{i,P} 
\end{array}
\right )
\quad \mu = \left (
\begin{array}{c}
\mu_{1} \\
\mu_2 \\
... \\
\mu_P 
\end{array}
\right )
\quad
\Sigma = \left (
\begin{array}{cccc}
\sigma_{11} & \sigma_{12} & ... & \sigma_{1P} \\
\sigma_{21} & \sigma_{22} & ... & \sigma_{2P} \\
... & ... & ... & ... \\
\sigma_{P1} & \sigma_{P2} & ... & \sigma_{PP} \\
\end{array}
\right )_{P\times P}
\end{aligned}
$$
$\Sigma$: [正定](./线性代数内容.org)的 （一般为半正定）
##  概率密度角度下的高斯分布
$\left( x - \mu \right)^T \Sigma^{-1} \left( x - \mu \right)$ ： $x$ 与 $\mu$ 之间的[马氏距离(Mahalanobis Distance)](./数学基础.org)。
由于 $\Sigma$ 为正定矩阵, 因此
$$
\begin{aligned}
\Sigma &= U \Lambda U^T \\
U U^T &= U^T U = I
\end{aligned}
$$
$\Lambda$ 为对角矩阵, $\lambda_i$ 为特征值， $u_i$ 为特征向量
$$
\begin{aligned}
\Lambda &= \left( \lambda_i \right) \quad i = 1,2,...,P\\
U &= \left( u_1, u_2, ..., u_p \right)_{P \times P}
\end{aligned}
$$
$$
\begin{aligned}
\Sigma &= U \Lambda U^T \\
& = (u_1, u_2, ..., u_p) \left (
\begin{array}{cccc}
\lambda_{1} & ... & ... & 0 \\
... & \lambda_2 & ... & ... \\
... & ... & ... & ... \\
0 & ... & ... & \lambda_P
\end{array}
\right ) \left (
\begin{array}{c}
u_{1}^{T} \\
u_2^T \\
... \\
u_p^T
\end{array}
\right )\\
&= \sum\limits_{i=1}^P u_i \lambda_i u_i^T
\end{aligned}
$$

$$
\begin{aligned}
\Sigma^{-1} &= \left( U \Lambda U^T \right)^{-1} = \left( U^T \right)^{-1} \Lambda^{-1} U^{-1} = U \Lambda^{-1} U^T \\
&= \sum\limits_{i=1}^P u_i \frac{1}{\lambda_i} u_i^T
\end{aligned}
$$
$$
\begin{aligned}
\Delta &= \left( x - \mu  \right) \Sigma ^{ -1} \left( x -\mu \right)\\
&= \left( x -\mu \right)^T \sum\limits_{i=1}^P u_i \frac{1}{\lambda_i } u_i^T \left(  x - \mu  \right)\\
&= \sum\limits_{i=1}^P \left(  x -\mu \right)^T u_i \frac{1}{\lambda_i} u_i^T \left( x - \mu \right)\\
&= \sum\limits_{i=1}^P y_i \frac{1}{\lambda_i} y_i^T \\
&= \sum\limits_{i=1}^P \frac{y_i^2}{\lambda_i}
\end{aligned}
$$
其中 $y_i = \left( x -\mu \right)^T \mu_i$。
###  概率密度函数的形状
在二维情况下，当概率密度固定的情况下，高斯分布所有情况呈现为椭圆形状。
在三维情况下，当概率密度固定的情况下，高斯分布所有情况呈现为椭球状。
在更高维度情况下，当概率密度固定的情况下，高斯分布所有情况呈现为椭超球形状。
$\lambda_1,...\lambda_p$ 为 $\Sigma$ 的特征变量, $\frac{1}{\lambda_1}, ..., \frac{1}{\lambda_P}$ 其为对应椭球对应的参数。
##  已知联合概率求条件概率以及边缘概率
###  问题定义
- 已知：
$$
X = \left (
\begin{array}{c}
x_a \\
x_b 
\end{array}
\right )
\quad m+n = p
\quad \mu = \left (
\begin{array}{c}
\mu_{a} \\
\mu_b
\end{array}
\right )
\quad \Sigma = \left (
\begin{array}{cc}
\Sigma_{aa} & \Sigma_{ab} \\
\Sigma_{ba} & \Sigma_{bb}
\end{array}
\right )
$$
- 求:
$$
P \left( x_a \right), P \left( x_b | x_a \right) 
$$
$$
P \left( x_{b} \right), P \left( x_a | x_b \right)
$$
###  公式推导
####  $X_a \sim \mathcal N \left( \mu_a, \Sigma_{aa} \right)$ 推导
$$
x_a = \left( I_m \quad 0 \right) \left (
\begin{array}{c} 
x_{a} \\
x_b
\end{array}
\right)
$$
由于 $X$ 服从正态分布, $x_a = (I_m \quad 0) \cdot X$, 因此根据[定理1](./数学基础.org)可以得到:
$$
E \left[ x_a \right] = \left( I_m \quad 0 \right) \left (
\begin{array}{c}
\mu_{a} \\
\mu_b
\end{array}
\right )
= \mu_a
$$
$$
\begin{aligned}
Var \left[ x_a \right] &= \left( I_m \quad 0 \right) \left (
\begin{array}{cc}
\Sigma_{aa} & \Sigma_{ab} \\
\Sigma_{ba} & \Sigma_{bb} 
\end{array}
\right )
\left (
\begin{array}{c}
I_{m} \\
0
\end{array}
\right )\\
&= (\Sigma_{aa} \quad \Sigma_{ab})\left (
\begin{array}{c}
I_{m} \\
0
\end{array}
\right ) = \Sigma_{aa}
\end{aligned}
$$
最终可以得出
$X_a \sim \mathcal N \left( \mu_a, \Sigma_{aa} \right)$

####  $P \left( x_b | x_a \right)$ 推导
首先构造 $x_{b\cdot a} = x_b - \Sigma_{ba} \Sigma_{aa}^{-1} x_a$
根据文末[定理1](###定理1)可以得出 $x_{b\cdot a} \sim \mathcal N \left( \mu_{b\cdot a}, \Sigma_{bb\cdot a} \right)$
根据 $X_a \sim \mathcal N \left( \mu_a, \Sigma_{aa} \right)$ 得出 $x_{a} \sim \mathcal N \left(\mu_{a}, \Sigma_{aa} \right), x_{b} \sim \mathcal N \left( \mu_b, \Sigma_{bb} \right)$, 因此可以进一步得出:
$$
\mu_{b\cdot a} = E \left[ x_{b\cdot a} \right] = E \left( x_b - \Sigma_{ba} \Sigma_{aa}^{-1} x_a \right)  = E \left[ x_b \right] - \Sigma_{ba} \Sigma_{aa}^{-1} E \left[ x_a \right] = \mu_b - \Sigma_{ba} \Sigma_{aa}^{-1} \mu_{a}
$$
$$
\begin{aligned}
\Sigma_{bb\cdot a} &= Var \left[ x_{b\cdot a} \right] = Var \left[ x_b - \Sigma_{ba} \Sigma_{aa}^{-1} x_a \right] = Var \left[ x_b \right] - Var \left[ \Sigma_{ba} \Sigma_{aa}^{-1} x_a \right] = \Sigma_{bb} - \Sigma_{ba} \Sigma_{aa}^{-1} Var \left[ x_a \right] \left( \Sigma_{ba} \Sigma_{aa}^{-1} \right)^{T} \\
&= \Sigma_{bb} - \Sigma_{ba} \Sigma_{aa}^{-1} \Sigma_{aa} ( \Sigma_{aa}^{-1} )^{T} \Sigma{ ba }^{T} \\
&= \Sigma_{bb} -\Sigma_{ba} \Sigma_{aa}^{-1} \Sigma_{ab}
\end{aligned}
$$
因为 $x_b = x_{b\cdot a} + \Sigma_{ba} \Sigma_{aa}^{-1} x_a$ 并且 $x_{b\cdot a}$ 与 $x_a$ 独立, 因此结合[定理1]()可以得出:
- $x_b$ 服从高斯分布
- $E \left[ x_b | x_a \right] = \mu_{b\cdot a} + \Sigma_{ba}\Sigma_{aa}^{-1}x_a$
- $Var \left[ x_b | x_a \right] = Var \left[ x_{b\cdot a} \right] = \Sigma_{bb\cdot a}$ 
最后可以得到：
$$
x_b | x_a \sim \mathcal N \left( \mu_{b\cdot a} + \Sigma_{ba}\Sigma_{aa}^{-1}x_a, \Sigma_{bb\cdot a} \right)
$$

####  $x_{b\cdot a}$ 与 $x_a$ 之间的独立性证明
$$
\begin{aligned}
x_{b\cdot a} &= x_b - \Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
&= \left( -\Sigma_{ba}\Sigma_{aa}^{-1} \quad I \right) \left (
\begin{array}{c}
x_{a} \\
x_b
\end{array}
\right )
\end{aligned}
$$
$$
x_{a} = \left( I \quad 0 \right) \left (
\begin{array}{c}
x_{a} \\
x_b
\end{array}
\right )
$$

$$
\therefore M \Sigma N^T = (-\Sigma_{ba}\Sigma_{aa}^{-1} \quad I) \left (
\begin{array}{cc}
\Sigma_{aa} & \Sigma_{ab} \\
\Sigma_{ba} & \Sigma_{bb} 
\end{array}
\right )
\left (
\begin{array}{c}
I \\
0
\end{array}
\right )
= (0 \quad \Sigma_{bb} - \Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}) \left (
\begin{array}{c}
I \\
0
\end{array}
\right)
= 0
$$


根据文末[定理2](###定理2)可以得出 $x_{b\cdot a} \perp x_a$ 
$$
\therefore x_{b\cdot a} | x_a = x_{b\cdot a} 
$$
$$
x_b | x_a = x_{b\cdot a} | x_a - \Sigma_{ba}\Sigma_{aa}^T x_a | x_a = x_{b\cdot a} - \Sigma_{ba}\Sigma_{aa}^{-1} x_a
$$

##  已知边缘与条件概率求联合概率分布
###  问题定义
- 已知: 
$$
\begin{aligned}
P \left( x \right) &= \mathcal N \left( x | \mu, \Lambda^{-1} \right)\\
P \left( y | x \right) &= \mathcal N \left( y | Ax + b, L^{-1}  \right)
\end{aligned}
$$
- 求:
$$
P \left( y \right), P \left( x | y \right)
$$
###  公式推导
####  $P \left( y \right)$ 推导
$x, y, \varepsilon \sim r.v$, 并且$\varepsilon \perp x$。
$$
\begin{aligned}
y &= Ax + b + \varepsilon\\
\varepsilon &\sim \mathcal N \left( 0, L^{-1} \right) \\
\end{aligned}
$$
$$
\begin{aligned}
E \left[ y \right] &= E \left[ Ax + b + \varepsilon \right] = E \left[ Ax + b \right] + E \left[ \varepsilon \right] = A\mu + b\\ 
Var \left[ y \right] &= Var \left[ Ax + b +\varepsilon \right] = Var \left[ Ax + b \right] + Var \left[ \varepsilon \right] = A \Lambda^{-1} A^{T} + L^{-1}
\end{aligned}
$$

####  $P(x|y)$ 推导
$$
Z = \left ( 
\begin{array}{c}
x \\
y
\end{array}
\right )
\sim
\mathcal N \left ( \left [
\begin{array}{c}
\mu \\
A \mu + b 
\end{array}
\right ], \left [
\begin{array}{cc}
\Lambda^{-1} & \Delta \\
\Delta & L^{-1} + A \Lambda^{-1} \\ 
\end{array}
\right ] \right )
$$

$$
\begin{aligned}
\Delta &= Cov \left( x, y \right)\\
&= E \left[ (x - E [x])\cdot (y - E [y])^{T} \right]\\
&= E \left[ \left( x -\mu \right) \left( y - A\mu -b \right)^T \right]\\
&= E \left[ \left( x -\mu \right) \left( Ax + b + \varepsilon - A\mu - b \right) \right] \\
&= E \left[ \left( x -\mu \right) \left( Ax - A\mu + \varepsilon \right)^T \right]\\
&= E \left[ \left( x -\mu \right) \left( x -\mu \right)^T A^T + \left( x-\mu \right)\varepsilon \right] \\
&= E \left[ \left( x - \mu \right ) \left( x - \mu \right)^T \cdot A^T \right] + E \left[ \left( x - \mu \right) \varepsilon^T \right] \\
&= E \left[ \left( x - \mu \right) \left( x - \mu \right)^T \right ] A^T + E \left[ (x - \mu) \right] E \left[ \varepsilon \right] \quad \leftarrow \because x \perp \varepsilon\\ 
& = Cov \left[ x \right] A^T + E \left[ \left( x -\mu \right) \right] * 0 \\
& = \Lambda^{-1} A^T
\end{aligned}
$$
而后再根据$P \left( x_b | x_a \right)$可以得出 $P(x|y)$。

##  参数估计
###  Maximum likelihood estimation (MLE)
\[
\theta_{MLE} = \arg\max_{\theta} P(X|\theta)
\]
当 $P = 1$, $\theta = (\mu, \sigma^{2})$

###  公式推导
$$
\begin{aligned}
\log P(X|\theta) &= log \sum_{i=1}^N P(x_i|\theta) = \sum\limits_{i=1}^N \log P(x_i|\theta)\\
&= \sum\limits_{i = 1}^N \log \frac{1}{\sqrt{2\pi}\sigma} \exp( -\frac{(x_i-\mu)}{2\sigma^{2}})\\
&= \sum\limits_{i=1}^N \left[ \log \frac{1}{ \sqrt{2\pi}} + log \frac{1}{\sigma} - \frac{\left( x_i -\mu \right)^2}{2\sigma^2} \right]
\end{aligned}
$$

$\mu_{MLE}$ 是无偏估计, $\sigma_{MLE}$ 是有偏估计。
####  $\mu_{MLE}$ 推导 
$$
\begin{aligned}
\mu_{MLE} &= \arg \max_{\mu} \log P(X|\theta) \\
&= \arg \max_{\mu} \sum\limits_{i=1}^N {-\frac{\left( x_i -\mu \right)^2}{2\sigma^2}}\\
&= \arg \min_{\mu} \sum\limits_{i=1}^N {\left( x_i - \mu \right)^2}
\end{aligned}
$$
$$
\begin{aligned}
\frac{\partial}{\partial \mu} \sum \left( x_i - \mu \right)^2 &= \sum\limits_{i=1}^N 2*\left( x_i - \mu \right)*(-1) = 0\\
\sum\limits_{i=1}^N \left( x_i - \mu \right) &= 0 \\
\sum\limits_{i=1}^N x_i - \sum\limits_{i=1}^N \mu &= 0 \\
N*\mu = \sum\limits_{i=1}^N x_i &\\
\mu_{MLE} = \frac{1}{N} \sum\limits_{i=1}^N x_i & 
\end{aligned}
$$
\[
E \left( \mu_{MLE} \right) = \frac{1}{N} \sum\limits_{i=1}^N E[x_i]  = \frac{1}{N} \sum\limits_{i=1}^{N} \mu = \mu 
\]

####  $\sigma_{MLE}$ 推导
$$
\begin{aligned}
\sigma_{MLE}^2 &= \arg \max_{\sigma} P(X|\theta) \\
&= \arg\max_{\sigma} \sum\limits_{i=1}^N (- \log \sigma - \frac{\left( x_i-\mu_i \right)^2}{2\sigma^2})
\end{aligned}
$$
$$
\begin{aligned}
&\mathcal L(\sigma) =  - \log \sigma - \frac{\left( x_i-\mu_i \right)^2}{2\sigma^2} \\
&\frac{\partial \mathcal L}{\partial\sigma} = \sum\limits_{i=1}^N \left[ -\frac{1}{\sigma} + \sigma^{-3} \left( x_i -\mu \right)^{2}\right] \\
&\sum\limits_{i=1}^N \left[ -\sigma^2 + \left( x_i -\mu \right)^2  \right] = 0\\
& -N\sigma^2 + \sum\limits_{i=1}^N \left( x_i -\mu \right)^2 = 0 \\
& \sigma_{MLE}^2 = \frac{1}{N} \sum\limits_{i=1}^N \left( x_i - \mu_{MLE} \right)^{2}
\end{aligned}
$$
$$\begin{aligned}
\sigma_{MLE}^{2} &= \frac{1}{N} \sum\limits_{i=1}^N \left( x_i - \mu_{MLE} \right)^{2} = \frac{1}{N} \sum\limits_{i=1}^N \left( x_i^2 - 2x_i \mu_{MLE} + \mu_{MLE}  \right) \\
&= \frac{1}{N} \sum\limits_{i=1}^N x_i^2 - \frac{1}{N} \sum\limits_{i=1}^N 2 x_i \mu_{MLE} + \frac{1}{N} \sum\limits_{i=1}^N \mu_{MLE}^2  \\
&= \frac{1}{N} \sum\limits_{i=1}^N x_i^2 - 2 \mu_{MLE}^2 + \mu_{MLE}^2 \\
&= \frac{1}{N} \sum\limits_{i=1}^N x_i^2 - \mu_{MLE}^{2} 
\end{aligned}
$$
$$
Var(\mu_{MLE}) = Var(\frac{1}{N}\sum\limits_{i=1}^N x_i) = \frac{1}{N^2} \sum\limits_{i=1}^N Var(x_i) = \frac{1}{N} Var(x_i) = \frac{1}{N} \sigma^2
$$
$$
\begin{aligned}
E[\sigma_{MLE}^2] &= E[\frac{1}{N} \sum\limits_{i=1}^N x_i^2 - \mu_{MLE}^2] = E[(\frac{1}{N}\sum\limits_{i=1}^N x_i^2 - \mu^2) - \left( \mu_{MLE}^2 -\mu^2 \right)] \\
&= E[\frac{1}{N} \sum\limits_{i=1}^N x_i^2 -\mu^2] - E(\mu_{MLE}^2 - \mu^2)\\
&= [\frac{1}{N} \sum\limits_{i=1}^N E(x_i^2 - \mu^2)] - [E(\mu_{MLE}^2) - E(\mu^2)]\\
&= [\frac{1}{N} \sum\limits_{i=1}^N (E(x_i^2) - \mu^2)] - [E(\mu_{MLE}^2) - \mu^2] \\
&= [\frac{1}{N} \sum\limits_{i=1}^N (Var(x_i))] - [E(\mu_{MLE}^2) - E(\mu_{MLE}^2)^{2}]\\
&= [\sigma^{2}] - [Var(\mu_{MLE})]\\
&= [\sigma^2] - [\frac{1}{N} \sigma^2]\\
&= \frac{N-1}{N} \sigma^2
\end{aligned}
$$

\[
E(\sigma_{MLE}) = \frac{N-1}{N} \sigma^2 
\]

\[
\sigma^{2} = \frac{1}{N-1} \sum\limits_{i=1}^N \left( x_i - \mu_{MLE} \right)^{2}
\]

##  高斯分布的局限性
###  高纬度的情况下参数过多
$\Sigma_{P\times P}$ 为对角矩阵， 所以参数个数为 $\frac{P \left( P+1 \right)}{2}$, 为 $O(P^2)$ 量级。
解决办法，对 $\Sigma$ 进行假设。
假设其为对角矩阵，对应体现在 [Factor analysis](./Factor%20Analysis.org) 模型中。 
交涉其为对角矩阵，并且各个值相等（ **各向同性** ）,对应体现在[Factor Analysis](./Factor%20Analysis.org) 模型中。

###  单个高斯分布无法
可以使用多个高斯模型，对应体现在 [混合模型](./Factor%20Analysis.org) 中。

## 定理
### 定理1
已知：
$$
\begin{aligned}
x &\sim \mathcal{N} \left( \mu, \Sigma \right) \quad x \in \mathbb R^{p} \\
y &= A x + B \quad y\in \mathbb R^{q}
\end{aligned}
$$
则:
$$
\begin{aligned}
y &\sim \mathcal N \left( A \mu + B, A \Sigma A^T  \right) \\
E \left[ y \right] &= E \left[ Ax + B \right] = A * E \left[ x \right] + B = A \mu + B \\
Var \left[ y \right] &= Var \left[ Ax + B \right] = Var \left[ Ax \right] + Var \left[ B \right]  = A \cdot Var \left[ x \right] \cdot A^T = A \cdot \Sigma \cdot A^{T}
\end{aligned}
$$
### 定理2
若 $x \sim \mathcal N \left( \mu, \Sigma \right)$, 则 $Mx \perp Mx \Leftrightarrow M\Sigma N^T = 0$
- 证明
$$
\begin{aligned}
&\because x \sim \mathcal N \left( \mu, \Sigma \right)\\
&\therefore Mx \sim \mathcal N \left( M\mu, M\Sigma M^T \right)\\
& \quad My \sim \mathcal N \left( N\mu, N\Sigma N^T \right)\\
&\therefore Cov(Mx, Nx) \\
&= E \left[ (Mx - M\mu) (Nx - N\mu)^T \right] \\
&= E \left[ M (x-\mu) \left( N(x-\mu) \right)^T \right] \\
&= E \left[ M \left( x -\mu \right) \left( x -\mu \right)^T N^T \right]\\
&= M \cdot E\left[ \left( x-\mu \right) \left( x-\mu \right)^T \right] \cdot N^T\\
&= M\Sigma N^T
\end{aligned}
$$
$\because Mx \perp Nx$ 且均为高斯分布
$\therefore Cov \left( Mx, Nx \right) = M \Sigma N^T = 0$

##  待思考问题
通过三维高斯分布的概率密度函数，来思考联合概率分布边缘概率分布之间的关系
杰森不等式的理解
