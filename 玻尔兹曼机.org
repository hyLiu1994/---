* 玻尔兹曼机
** 核心思想
为了解决 ~Hopfield Network~ 的问题而提出的。
~Hopfield Network~ 来源于统计物理学中的 ~Ising Model~ 。

** 模型定义
\begin{figure*}[htbp]
\centerline{\includegraphics[width = 0.3\textwidth]{./Figure/BoltzmannMachine.png}}
\end{figure*}
$L = [L_{ij}]_{D \times D}, J = [J_{ij}]_{P\times P}, W = [W_{ij}]_{D\times P}$

   \begin{align*}
\left \{
\begin{array}{l}
P (v,h) = \frac{1}{Z} \exp \left\{ - E \left( v, h \right) \right\} \\
E (v,h) = - \left( v^T w h + \frac{1}{2} v^T L v + \frac{1}{2} h^T J h  \right)\\
\end{array}
\right 
\end{align*}
** Inference log-likelihood and gradient
\begin{align*}
log-likelihood: P \left( v \right) = \sum\limits_{h} P \left( v,h \right)\\
\frac{1}{N} \sum\limits_{v \in V} \log P \left( v \right) \longleftarrow log-likelihood
\end{align*}

\begin{align*}
\frac{\partial}{\partial \theta} \frac{1}{N} \sum\limits_{v\in V} \log P \left( v \right) &= \frac{1}{N} \sum\limits_{v \in V} \frac{\partial \log P \left( v \right)}{\partial \theta}\\
&= \frac{1}{N} \sum\limits_{v\in V} \{ \sum\limits_{v} \sum\limits_h P \left( v,h \right) \frac{\partial E \left( v,h \right)}{\partial \theta} - \sum\limits_{h} P \left( h|v \right) \frac{\partial E \left( v,h \right)}{\partial \theta} \}\\
&= \frac{1}{N} \sum\limits_{v \in V} \{ \sum\limits_{v} \sum\limits_{h} P \left( v,h \right) - \left( v h^T \right) + \sum\limits_{h} P \left( h | v \right) + (v h^T) \}\\
&= \frac{1}{N} \sum\limits_{v \in V} \{ \sum\limits_{h} P \left( h | v \right) v h^T - \sum\limits_v \sum\limits_h P \left( v,h \right) v h^T \}\\
&= \frac{1}{N} \sum\limits_{v \in V} \sum\limits_h P \left( h | v \right) v h^T - \frac{1}{N} \sum\limits_{v\in V} \sum\limits_v \sum\limits_h P \left( v, h \right) v h^T\\
&= \frac{1}{N} \sum\limits_{v \in V} \sum\limits_h P \left( h|v \right) v h^T - \sum\limits_v \sum\limits_{h} P \left( v, h \right) v h^T\\
&= E_{P_{data}} [v h^T] - E_{P_{model}} [v h^T]\\
\end{align*} 

其中
\begin{align*}
&P_{data}(v,h) = P_{data} (v) P_{model} (h|v)\\
&P_{model}(v,h) = P_{model} (v, h)
\end{align*}

\begin{align*}
\Delta W = \alpha \left( E_{P_{data}} [v h^T] - E_{P_{model}} [v h^T] \right)\\
\Delta L = \alpha \left( E_{P_{data}} [v v^T] - E_{P_{model}} [v v^T] \right)\\
\Delta J = \alpha \left( E_{P_{data}} [h h^T] - E_{P_{model}} [h h^T] \right)
\end{align*} 

** 条件概率 $P \left( v_i = 1 | h, v_{-i} \right), P \left( h_j = 1 | v, h_{-j} \right)$ 推导 
\begin{align*}
P \left( v_i | h, v_{-i} \right) &= \frac{P \left( v,h \right)}{ P \left( h, v_{-i} \right)} = \frac{ \frac{1}{Z} \exp \left\{ - E \left( v, h \right) \right\} } { \sum\limits_{v_i} \frac{1}{Z} \exp \left\{ - E \left( v,h \right) \right\} } = \frac{\exp \left\{ v^T w h + \frac{1}{2} v^T L v + \frac{1}{2} h^T J h \right\} }{\sum\limits_{v_i} \exp \left\{ v^T w h + \frac{1}{2} v^T L v + \frac{1}{2} h^T J h \right\} }\\
&= \frac{\exp \left\{ v^T w h + \frac{1}{2} v^T L v \right\} \exp \left\{ \frac{1}{2} h^T J h \right\}  }{ \exp \left\{ \frac{1}{2} h^T J h \right\} \sum\limits_{v_i} \exp \left\{ v^T w h + \frac{1}{2} v^T L v \right\} } = \frac{\exp \left\{ v^T w h + \frac{1}{2}v^T L v \right\} |_{v_i = 1} }{\exp \left\{ v^T w h + \frac{1}{2} v^T L v \right\}|_{v_i = 0} + \exp \left\{ v^T w h + \frac{1}{2} v^T L v \right\}|_{v_i = 1} }\\
&= \frac{ \exp \left\{ \sum\limits_{\hat{i} = 1}^D \sum\limits_{j=1}^P v_{\hat{i}} w_{\hat{i}j} h_j  + \frac{1}{2} \sum\limits_{\hat{i} = 1/i}^D \sum\limits_{k=1/i}^D v_{\hat{i}} L_{\hat{i}k} v_k + \sum\limits_{j=1}^P w_{ij} h_j + \sum\limits_{k=1/i}^P L_{ik} v_k \right\} } {\exp \left\{ \sum\limits_{\hat{i} = 1}^D \sum\limits_{j=1}^P v_{\hat{i}} w_{\hat{i}j} h_j   + \frac{1}{2} \sum\limits_{\hat{i} = 1/i}^D \sum\limits_{k=1/i}^D v_{\hat{i}} L_{\hat{i}k} v_k \right\} + \exp \left\{ \sum\limits_{\hat{i} = 1}^D \sum\limits_{j=1}^P v_{\hat{i}} w_{\hat{i}j} h_j   + \frac{1}{2} \sum\limits_{\hat{i} = 1/i}^D \sum\limits_{k=1/i}^D v_{\hat{i}} L_{\hat{i}k} v_k + \sum\limits_{j=1}^P w_{ij} h_j + \sum\limits_{k=1/i}^P L_{ik} v_k \right\} }\\
&= \frac{\exp \left( \sum\limits_{j=1}^P w_{ij} h_j + \sum\limits_{k=1/i}^P L_{ik} v_k \right)}{ 1 + \exp \left\{ \sum\limits_{j=1}^P w_{ij} h_j + \sum\limits_{k=1/i}^P L_{ik} v_k \right\}}\\
&= \frac{1}{1+ \exp \left( - ( \sum\limits_{j=1}^P w_{ij} h_j + \sum\limits_{k=1/i}^P L_{ik} v_k) \right)}\\
&= \sigma (\sum\limits_{j=1}^P w_{ij} h_j + \sum\limits_{k=1/i}^P L_{ik} v_k)
\end{align*}

** 基于 MCMC 采集 Boltzmann Machine 隐变量
\begin{align*}
P \left( v_i = 1 | h, v_{-i} \right) &= \sigma \left( \sum\limits_{j=1}^D w_{ij} h_j + \sum\limits_{k=1/i}^D L_{ik} v_k \right)\\
P \left( h_j = 1 | v, h_{-j} \right) &= \sigma \left( \sum\limits_{i=1}^D w_{ij} v_i + \sum\limits_{m=1/j}^P J_{jm} h_m \right)
\end{align*}

可以利用以上条件概率，通过Gibbs 采样，采样所有的隐变量与观察量, 并利用以下公式求解梯度。

\begin{align*}
\Delta W = \alpha \left( E_{P_{data}} [v h^T] - E_{P_{model}} [v h^T] \right)\\
\Delta L = \alpha \left( E_{P_{data}} [v v^T] - E_{P_{model}} [v v^T] \right)\\
\Delta J = \alpha \left( E_{P_{data}} [h h^T] - E_{P_{model}} [h h^T] \right)
\end{align*} 
其中 $\alpha$ 为学习率。

** 基于平均场的变分推断
\begin{align*}
&\mathcal{L} = ELBO = \log P_{\theta} \left( v \right) - KL \left( q_{\phi} || P_{\theta} \right) = \sum\limits_h q_{\phi} \left( h | v \right) \log P_{\theta} \left( v,h \right) + H [q]\\
&q_{\phi} \left( h | v \right) = \prod\limits_{ j=1 }^ { P } q_{\phi} (h_j | v),  q_{\phi}\left( h_j = 1 | v \right) = \phi_j, \phi = \left\{ \phi_j \right\}_{j=1}^P\\
&H [q] = \sum\limits_{j=1}^P \phi_j \log \frac{1}{\phi_j} + (1 - \phi_j) \log \frac{1}{1 - \phi_j}
\end{align*}

\begin{align*}
\hat{\phi_j} &= \arg \max_{\phi_j} \mathcal{L} = \arg \max \sum\limits_{h} q_{\phi} \left( h | v \right) [-\log Z + v^T w h + \frac{1}{2} v^T L v + \frac{1}{2} h^T J h] + H \left[ q \right]\\
&= \arg \max_{\phi_j} \sum\limits_{h} q_{\phi} \left( h | v \right) \left[ - \log Z + \frac{1}{2} v^T L v \right] + \sum\limits_{h} q_{\phi} \left( h | v \right) \left[ v^T w h + \frac{1}{2} h^T J h \right] + H \left[ q \right]\\
&= \arg \max_{\phi_j} \sum\limits_{h} q_{\phi} \left( h | v \right) v^T w h + \frac{1}{2} q_{\phi} \left( h | v \right) h^T J h + H [q] = \arg \max \textcircled{1} + \textcircled{2} + \textcircled{3}
\end{align*} 


\begin{align*}
\textcircled{1} &= \sum\limits_{h} q_{\phi} \left( h | v \right) \sum\limits_{i=1}^D \sum\limits_{\hat{j}=1}^P v_i w_{ij} h_j = \sum\limits_h \prod\limits_{ \hat{j}=1 }^ { P } q_{\phi} \left( h_{\hat{j}} | v \right) \sum\limits_{i=1}^D \sum\limits_{\hat{j}=1}^P v_i w_{i \hat{j}} h_{\hat{j}} = \sum\limits_{i=1}^D \sum\limits_{\hat{j}=1}^D \phi_{\hat{j}} v_i w_{ij} \\
\textcircled{2} &= \sum\limits_{j=1}^P \sum\limits_{m=1/j}^P \phi_{\hat{j}} \phi_m J_{jm} + C\\
\textcircled{3} &= - \sum\limits_{j=1}^P \left[ \phi_j \log \phi_j + (1 - \phi_j) \log (1 - \phi_j) \right]
\end{align*}

\begin{align*}
&\frac{\partial \left[ \textcircled{1} + \textcircled{2} + \textcircled{3}\right]}{\partial \phi_j}  = 0\\
&\Longrightarrow \phi_j = \sigma \left( \sum\limits_{i=1}^D v_i w_{ij} + \sum\limits_{m=1/j}^P \phi_m J_{jm} \right)\\
& \hat{\phi} = \left\{ \hat{\phi}_j \right\}_{j=1}^P
\end{align*}

不动点方程， 利用坐标上升法求解。

