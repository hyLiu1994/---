#+LATEX_HEADER:\usepackage{ctex}
#+TITLE: 高斯分布
* 高斯分布
** 高斯分布公式
1. 一维高斯分布公式
\[
   P(x) = \frac{1}{\sqr{2\pi} \sigma} \exp (-\frac{(x-\mu)^2}{2\sigma^2})
 \]
2. 多维高斯分布公式
 
\begin{equation}
\label{eq:3}
 P(x) = \frac{1}{(2\pi)^{\frac{1}{P}}\left| \Sigma \right|^{\frac{1}{2}}} \exp (-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
\end{equation}
$\Sigma$ 为协方差矩阵

** 问题定义
数据与参数定义为如下形式:
\[
Data : X = (x_1,x_2,...,x_N)^T = \left (\begin{array}{c}
x_{1}^T \\
x_2^T \\
... \\
x_N^T
}
\end{array}
\right )_{N*P}
\]
\[
Parameter: \theta = (\mu,\Sigma)
\]

\[
x_i \in \mathbb{R}^{P}
\]
\begin{align}
\label{eq:18}
x_i \sim \mathcal{N}(\mu,\Sigma) = \frac{1}{\left( 2\pi \right)^{\frac{P}{2} \cdot \left| \Sigma \right|^{\frac{1}{2}}}} \exp \left( -\frac{1}{2} \left( x-\mu \right)^T \Sigma^{-1} \left( x - \mu \right) \right)
\end{align}
$\left( x - \mu \right)^T \Sigma^{-1} \left( x - \mu \right)$ 为[[file:%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%86%85%E5%AE%B9.org::*%E4%BA%8C%E6%AC%A1%E5%9E%8B][二次型]]。
\begin{align}
\label{eq:19}
x_i = \left ( 
\begin{array}{c}
x_{i,1} \\
x_{i,2} \\
... \\
x_{i,P} 
\end{array}
\right )
\quad \mu = \left (
\begin{array}{c}
\mu_{1} \\
\mu_2 \\
... \\
\mu_P 
\end{array}
\right )
\quad
\Sigma = \left (
\begin{array}{cccc}
\sigma_{11} & \sigma_{12} & ... & \sigma_{1P} \\
\sigma_{21} & \sigma_{22} & ... & \sigma_{2P} \\
... & ... & ... & ... \\
\sigma_{P1} & \sigma_{P2} & ... & \sigma_{PP} \\
\end{array}
\right )_{P\times P}
\end{align}
$\Sigma$: [[file:%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%86%85%E5%AE%B9.org::*%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5][正定]]的 （一般为半正定）

** 概率密度角度下的高斯分布
$\left( x - \mu \right)^T \Sigma^{-1} \left( x - \mu \right)$ ： $x$ 与 $\mu$ 之间的[[file:%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.org::*%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB(Mahalanobis%20Distance)][马氏距离(Mahalanobis Distance)]]。
由于 $\Sigma$ 为正定矩阵, 因此
\begin{align}
\label{eq:21}
\Sigma &= U \Lambda U^T \\
U U^T &= U^T U = I
\end{align}
$\Lambda$ 为对角矩阵, $\lambda_i$ 为特征值， $u_i$ 为特征向量
\begin{align}
\label{eq:22}
\Lambda &= \diag \left( \lambda_i \right) \quad i = 1,2,...,P\\
U &= \left( u_1, u_2, ..., u_p \right)_{P*P}
\end{align}
\begin{equation}
\label{eq:28}
\begin{align}
\Sigma &= U \Lambda U^T \\
& = (u_1, u_2, ..., u_p) \left (
\begin{array}{cccc}
\lambda_{1} & ... & ... & 0 \\
... & \lambda_2 & ... & ... \\
... & ... & ... & ... \\
0 & ... & ... & \lambda_P
\end{array}
\right ) \left (
\begin{array}{c}
u_{1}^{T} \\
u_2^T \\
... \\
u_p^T
\end{array}
\right )\\
&= \sum\limits_{i=1}^P u_i \lambda_i u_i^T
\end{align}
\end{equation}

\begin{equation}
\label{eq:27}
\begin{align}
\Sigma^{-1} &= \left( U \Lambda U^T \right)^{-1} = \left( U^T \right)^{-1} \Lambda^{-1} U^{-1} = U \Lambda^{-1} U^T \\
&= \sum\limits_{i=1}^P u_i \frac{1}{\lambda_i} u_i^T
\end{align}
\end{equation}
\begin{equation}
\label{eq:26}
\begin{align}
\Delta &= \left( x - \mu  \right) \Sigma ^{ -1} \left( x -\mu \right)\\
&= \left( x -\mu \right)^T \sum\limits_{i=1}^P u_i \frac{1}{\lambda_i } u_i^T \left(  x - \mu  \right)\\
&= \sum\limits_{i=1}^P \left(  x -\mu \right)^T u_i \frac{1}{\lambda_i} u_i^T \left( x - \mu \right)\\
&= \sum\limits_{i=1}^P y_i \frac{1}{\lambda_i} y_i^T \\
&= \sum\limits_{i=1}^P \frac{y_i^2}{\lambda_i}
\end{align}
\end{equation}
其中 $y_i &= \left( x -\mu \right)^T \mu_i$。

*** 自己的想法
在二维情况下，当概率固定的情况下，高斯分布所有情况呈现为椭圆形状。
在三维情况下，当概率固定的情况下，高斯分布所有情况呈现为椭球状。
在更高维度情况下，当概率固定的情况下，高斯分布所有情况呈现为椭超球形状。
$\lambda_1,...\lambda_p$ 为 $\Sigma$ 的特征变量, $\frac{1}{\lambda_1}, ..., \frac{1}{\lambda_P}$ 其为对应椭球对应的参数。

** 高斯分布的局限性
*** 高纬度的情况下参数过多
$\Sigma_{P\times P}$ 为对角矩阵， 所以参数个数为 $\frac{P \left( P+1 \right)}{2}$, 为 $O(P^2)$ 量级。
解决办法，对 $\Sigma$ 进行假设。
假设其为对角矩阵，对应体现在 [[file:Factor%20Analysis.org::*Factor%20analysis][Factor analysis]] 模型中。 
交涉其为对角矩阵，并且各个值相等（ *各向同性* ）,对应体现在 [[file:Factor%20Analysis.org::*P-PCA][P-PCA]] 模型中。

*** 单个高斯分布无法
可以使用多个高斯模型，对应体现在 [[file:Factor%20Analysis.org::*%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B][混合模型]] 中。

** 参数估计
*** Maximum likelihood estimation (MLE)
\[
\theta_{MLE} = \arg\max_{\theta} P(X|\theta)
\]
当 $P = 1$, $\theta = (\mu, \sigma^{2})$

*** 公式推导
\begin{equation}
\begin{align}
\label{eq:4}
\log P(X|\theta) &= log \sum_{i=1}^N P(x_i|\theta) = \sum\limits_{i=1}^N \log P(x_i|\theta)\\
&= \sum\limits_{i = 1}^N \log \frac{1}{\sqr{2\pi}\sigma} \exp( -\frac{(x_i-\mu)}{2\sigma^{2}})\\
&= \sum\limits_{i=1}^N \left[ \log \frac{1}{ \sqrt{2\pi}} + log \frac{1}{\sigma} - \frac{\left( x_i -\mu \right)^2}{2\sigma^2} \right]
\end{align}
\end{equation}

$\mu_{MLE}$ 是无偏估计, $\sigma_{MLE}$ 是有偏估计。
**** $\mu_{MLE}$ 推导 
\begin{equation}
\begin{align}
\label{eq:5}
\mu_{MLE} &= \arg \max_{\mu} \log P(X|\theta) \\
&= \arg \max_{\mu} \sum\limits_{i=1}^N {-\frac{\left( x_i -\mu \right)^2}{2\sigma^2}}\\
&= \arg \min_{\mu} \sum\limits_{i=1}^N {\left( x_i - \mu \right)^2}
\end{align}
\end{equation}
\begin{equation}
\label{eq:6}
\begin{align}
\frac{\partial}{\partial \mu} \sum \left( x_i - \mu \right)^2 &= \sum\limits_{i=1}^N 2*\left( x_i - \mu \right)*(-1) = 0\\
\sum\limits_{i=1}^N \left( x_i - \mu \right) &= 0 \\
\sum\limits_{i=1}^N x_i - \sum\limits_{i=1}^N \mu &= 0 \\
N*\mu = \sum\limits_{i=1}^N x_i &\\
\mu_{MLE} = \frac{1}{N} \sum\limits_{i=1}^N x_i & 
\end{align}
\end{equation}
\[
E \left( \mu_{MLE} \right) = \frac{1}{N} \sum\limits_{i=1}^N E[x_i]  = \frac{1}{N} \sum\limits_{i=1}^{N} \mu = \mu 
\]

**** $\sigma_{MLE}$ 推导
\begin{equation}
\begin{align}
\label{eq:2}
\sigma_{MLE}^2 &= \arg \max_{\sigma} P(X|\theta) \\
&= \arg\max_{\sigma} \sum\limits_{i=1}^N (- \log \sigma - \frac{\left( x_i-\mu_i \right)^2}{2\sigma^2})
\end{align}
\end{equation}
\begin{equation}
\begin{align}
\label{eq:8}
&\mathcal L(\sigma) =  - \log \sigma - \frac{\left( x_i-\mu_i \right)^2}{2\sigma^2} \\
&\frac{\partial \mathcal L}{\partial\sigma} = \sum\limits_{i=1}^N \left[ -\frac{1}{\sigma} + \sigma^{-3} \left( x_i -\mu \right)^{2}\right] \\
&\sum\limits_{i=1}^N \left[ -\sigma^2 + \left( x_i -\mu \right)^2  \right] = 0\\
& -N\sigma^2 + \sum\limits_{i=1}^N \left( x_i -\mu \right)^2 = 0 \\
& \sigma_{MLE}^2 = \frac{1}{N} \sum\limits_{i=1}^N \left( x_i - \mu_{MLE} \right)^{2}
\end{align}
\end{equation}
\[
\sigma_{MLE}^{2} = \frac{1}{N} \sum\limits_{i=1}^N \left( x_i - \mu_{MLE} \right)^{2} = \frac{1}{N} \sum\limits_{i=1}^N \left( x_i^2 - 2x_i \mu_{MLE} + \mu_{MLE}  \right)\\
= \frac{1}{N} \sum\limits_{i=1}^N x_i^2 - \frac{1}{N} \sum\limits_{i=1}^N 2 x_i \mu_{MLE} + \frac{1}{N} \sum\limits_{i=1}^N \mu_{MLE}^2  
= \frac{1}{N} \sum\limits_{i=1}^N x_i^2 - 2 \mu_{MLE}^2 + \mu_{MLE}^2 = \frac{1}{N} \sum\limits_{i=1}^N x_i^2 - \mu_{MLE}^{2} 
\]
\[
Var(\mu_{MLE}) = Var(\frac{1}{N}\sum\limits_{i=1}^N x_i) = \frac{1}{N^2} \sum\limits_{i=1}^N Var(x_i) = \frac{1}{N} Var(x_i) = \frac{1}{N} \sigma^2
\]
\begin{equation}
\begin{align}
\label{eq:9}
E[\sigma_{MLE}^2] &= E[\frac{1}{N} \sum\limits_{i=1}^N x_i^2 - \mu_{MLE}^2] = E[(\frac{1}{N}\sum\limits_{i=1}^N x_i^2 - \mu^2) - \left( \mu_{MLE}^2 -\mu^2 \right)] \\
&= E[\frac{1}{N} \sum\limits_{i=1}^N x_i^2 -\mu^2] - E(\mu_{MLE}^2 - \mu^2)\\
&= [\frac{1}{N} \sum\limits_{i=1}^N E(x_i^2 - \mu^2)] - [E(\mu_{MLE}^2) - E(\mu^2)]\\
&= [\frac{1}{N} \sum\limits_{i=1}^N (E(x_i^2) - \mu^2)] - [E(\mu_{MLE}^2) - \mu^2] \\
&= [\frac{1}{N} \sum\limits_{i=1}^N (Var(x_i))] - [E(\mu_{MLE}^2) - E(\mu_{MLE}^2)^{2}]\\
&= [\sigma^{2}] - [Var(\mu_{MLE})]\\
&= [\sigma^2] - [\frac{1}{N} \sigma^2]\\
&= \frac{N-1}{N} \sigma^2
\end{align}
\end{equation}

\[
E(\sigma_{MLE}) = \frac{N-1}{N} \sigma^2 
\]

\[
\sigma^{2} = \frac{1}{N-1} \sum\limits_{i=1}^N \left( x_i - \mu_{MLE} \right)^{2}
\]

