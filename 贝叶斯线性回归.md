# 贝叶斯线性回归 (Bayesian Linear Regression)
## 问题定义
数据的表示形式如下:
$$
\mathbf{X} = [\mathbf{x_1}, \mathbf{x_2}, \cdots, \mathbf{x_N}]^T
=\left[\begin{array}{c}
\mathbf{x_1^T} \\
\mathbf{x_2^T} \\
\vdots\\
\mathbf{x_N^T} 
\end{array}\right] = \left (
\begin{array}{ccc}
x_{11} & ... & x_{1p} \\
\vdots & \ddots & \vdots \\
x_{N1} & ... & x_{Np}\\
\end{array}
\right )
$$
$$
\mathbf{Y} = \left[\begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_N
\end{array}\right]
$$
## 核心思想
利用贝叶斯学派思想解决线性回归问题.
## Model 
$$
\left \{ 
\begin{array}{l}
f \left( x \right) = w^T x = x^T w\\
y = f \left( x \right) + \epsilon
\end{array}
\right.
$$
其中 x,y,$\epsilon$ are r.v, $\epsilon \sim \mathcal{N} \left( 0, \sigma^2 \right)$ 
## Inference
### 后验函数 $P \left( w | Data \right)$ Inference
$$
\begin{aligned}
P \left( w| Data \right) &= p \left( w | X, Y \right) = \frac{P \left( w, Y | X \right)}{P \left( Y |X \right)} = \frac{P \left( Y| w, X \right) P \left( w \right)}{ \int P \left( Y| w,X \right) P \left( w \right) d w}\\
P \left( Y | w, X \right) &= \prod\limits_{ i=1 }^ { N } P \left( y_i | w, x_i \right) = \prod\limits_{ i=1 }^ { N }  \mathcal{N} \left( y_i | w^T x_i , \sigma^2 \right)\\
P \left( y | x, w \right) &= \mathcal{N} \left( w^T x, \sigma^2 \right), \quad P \left( w  \right) = \mathcal{N} \left( 0, \Sigma_p \right)\\
\end{aligned}
$$
$$
\begin{aligned}
\underbrace{P \left( w | Data \right)}_{Gaussian} &\propto \underbrace{P \left( Y | w, X \right)}_{Gaussian} \cdot  \underbrace{P \left( w \right)}_{Gaussian}\\
&\propto \prod\limits_{ i=1 }^ { N }  \mathcal{N} \left( y_i | w^T x_i, \sigma^2 \right) \mathcal{N} \left( 0, \Sigma_p \right)\\
&\propto \mathcal{N} \left( \mu_w, \Sigma_w \right)
\end{aligned}
$$

### 似然函数 $P \left( Y | X, w \right)$ Inference
$$
\begin{aligned}
P \left( Y | X, w \right) &= \prod\limits_{ i=1 }^ { N } \frac{1}{(2 \pi)^{\frac{1}{2}} \sigma} \exp \left\{ - \frac{1}{2 \sigma^2} \left( y_i - w^T x_i \right)^2 \right\}\\
P \left( Y | X, w \right) &= \frac{1}{(2 \pi)^{\frac{1}{2}} \sigma} \exp \left\{ - \frac{1}{2 \sigma^2} \sum\limits_{ i=1 }^ { N } \left( y_i - w^T x_i \right)^2 \right\}\\
P \left( Y | X, w \right) &= \frac{1}{(2 \pi)^{\frac{1}{2}} \sigma} \exp \left\{ - \frac{1}{2} \left( Y - X w \right) \sigma^2 I \left( Y - X w \right) \right\}\\
P \left( Y | X, w \right) &= \mathcal{N} \left( Xw, \sigma^{-2} I \right)
\end{aligned}
$$

### 后验函数 $P \left( w | Data \right)$ 参数 $\mu_w$, $\Sigma_w$ Inference
$$
\begin{aligned}
\underbrace{P \left( w | Data \right)}_{Gaussian} &\propto \underbrace{P \left( Y | w, X \right)}_{Gaussian} \cdot  \underbrace{P \left( w \right)}_{Gaussian}\\
&\propto \prod\limits_{ i=1 }^ { N }  \mathcal{N} \left( y_i | w^T x_i, \sigma^2 \right) \mathcal{N} \left( 0, \Sigma_p \right)\\
&\propto \mathcal{N} \left( \mu_w, \Sigma_w \right)\\
&\propto \mathcal{N} \left( X w, \sigma^{-2} I \right) \mathcal{N} \left( 0, \Sigma_p \right)\\
&\propto \exp \left\{ -\frac{1}{2} \left( Y - Xw \right)^T \sigma^{-2} I \left( Y - Xw \right) \right\}  \exp \left\{ - \frac{1}{2} w^T \Sigma_p^{-1} w \right\}\\
&= \exp \left\{ - \frac{1}{2 \sigma^2} \left( Y^T - w^T x^T \right) \left( Y - X w \right) - \frac{1}{2} w^T \Sigma^{-1}_p w \right\}\\
&= \exp \left\{ - \frac{1}{2 \sigma^2} \left( Y^T Y - 2 Y^T x w + w^T x^T x w \right) - \frac{1}{2} w^T \Sigma^{-1}_p w \right\}
\end{aligned}
$$

二次项: $- \frac{1}{2\sigma^2} w^T x^T x w - \frac{1}{2} w^T \Sigma_p^{-1} w = - \frac{1}{2} \left( w^T \left( \sigma^{-2} x^T x + \Sigma_p^{-1} \right) w \right)$
一次项: $- \frac{1}{2 \sigma^2} (-2) Y^T X w = \sigma^{-2} Y^T X w$

$$
\begin{aligned}
\Sigma_w^{-1} &= A = \left( \sigma^{-2} x^T x + \Sigma_p^{-1} \right)\\
\mu_w^T A &= \mu_w^T \Sigma_w^{-1} = \sigma^{-2} Y^T w \\
A \mu_w &= \sigma^{-2} x^T Y\\
\mu_w &= \sigma^{-2} A^{-1} X^T Y\\
\end{aligned}
$$

### Prediction Inference
$$
\begin{aligned}
&\because f(x) = w^T x, \quad w \sim \mathcal{N} \left( \mu_w, \Sigma_w \right)\\
&\therefore f(x^{* }) = x^{* T} w, \quad w \sim \mathcal{N} \left( x^{* T} \mu_w, x^{* T} \Sigma_w x^{* } \right)\\
&\because y^{* } = f(x^{* }) + \epsilon, \quad f(x^{* }) = x^{* T} w \sim \mathcal{N} \left( x^{* T} \mu_w, x^{* T} \Sigma_w x^{* } \right)\\
&\therefore P \left( y^{* } | Data, x^{* } \right) = \mathcal{N} \left( x^{* T} \mu_w, x^{* T} \Sigma_w x^{* } + \sigma^2 \right) \\
\end{aligned}
$$

$$
\begin{aligned}
P \left( y^{*} | x^{* }, Data \right) = \int P (y^{* }, w | x^{* }, Data) dw = \int P (w | Data) P (y^{* }, w | x^{* })) dw
\end{aligned}
$$
### 概率知识点
1. Gaussian 分布是自共轭的
## 讨论
- 回忆最小二乘法
- MAP是贝叶斯学派模型嘛
贝叶斯学派通常 将隐变量，参数等未知部分表达为分布形式。
MAP 会求出一个具体的值，属于点估计，因此这里认为MAP不属于贝叶斯学派的算法。
$$
w_{map} = \arg \max_{w} p(\text{Data}|w) p(w)
$$
- 共轭分布及其性质
共轭分布描述的是先验分布，后验分布两个具体概率分布的分布表达形式一致，但是参数不一致的现象。
标准描述如下：
在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布和后验分布称为共轭分布，而先验分布称为似然函数的共轭先验。
- 频率学派与贝叶斯学派的区别
贝叶斯学派 通常 将隐变量，参数等未知部分表达为分布形式。
频率学派 通常 估计出参数，隐变量的具体值。
